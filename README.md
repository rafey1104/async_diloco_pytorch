# üöÄ Momentum Look-Ahead for Asynchronous DiLoCo ‚Äì PyTorch Implementation

This project is a research-oriented PyTorch implementation of the paper:

> **"Momentum Look-Ahead for Asynchronous Distributed Low-Communication Training"**  
> *Published at ICLR 2025*  
> [arXiv Link](https://openreview.net/pdf?id=4O8nzTkHPI)

We simulate asynchronous training with 4 local workers and a central parameter server using LeNet on the CIFAR-10 dataset.

---

## üìå Features

- ‚úÖ Asynchronous worker training (no synchronization)
- ‚úÖ Central server applies **Momentum Look-Ahead** updates (per paper Eq. 7)
- ‚úÖ Modular architecture with real-time threading
- ‚úÖ Clean, research-style code (easy to extend to multi-machine)

---

## üß† Paper Idea in Short

Instead of waiting for all workers to sync, each one sends updates (‚àÜŒ∏) asynchronously.  
The server uses:

![Training Screenshot](assets/screenshot.png)

This approach improves convergence in heterogeneous hardware setups.

---

## üìÅ Project Structure

```bash
async_diloco_pytorch/
‚îú‚îÄ‚îÄ main.py # Launches async training and evaluation
‚îú‚îÄ‚îÄ models/
‚îÇ ‚îî‚îÄ‚îÄ lenet.py # LeNet-5 CNN model for CIFAR-10
‚îú‚îÄ‚îÄ datasets/
‚îÇ ‚îî‚îÄ‚îÄ cifar_loader.py # CIFAR-10 DataLoader
‚îú‚îÄ‚îÄ server/
‚îÇ ‚îî‚îÄ‚îÄ async_server.py # Momentum Look-Ahead server logic
‚îú‚îÄ‚îÄ workers/
‚îÇ ‚îî‚îÄ‚îÄ async_worker.py # Async local training worker (threaded)
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Installation

```bash
git clone https://github.com/rafey1104/async_diloco_pytorch.git
```

```bash
conda create -n async-diloco-env python=3.10 -y
conda activate async-diloco-env
pip install torch torchvision matplotlib
```

---

## üöÄ Usage

```bash
python main.py
```bash

> The default configuration runs for **30 seconds** with 4 workers.

---

## üîç Example Output

```bash
[Server] Running...
[Worker 0] Starting.
[Worker 1] Starting.
[Worker 2] Starting.
[Worker 3] Starting.

[Main] Running training for 30 seconds...
[Server] Shutting down.
[Eval] Accuracy: 31.59%
```

---

## üõ† Configuration (in `main.py`)

| Param         | Meaning                            | Example Value |
|---------------|------------------------------------|---------------|
| `NUM_WORKERS` | Number of async threads            | 4             |
| `INNER_STEPS` | Local SGD steps before sending     | 5             |
| `SERVER_LR`   | Learning rate at server            | 0.7           |
| `MOMENTUM`    | Œ≥ for momentum buffer              | 0.9           |
| `TRAIN_SECONDS`| How long to train asynchronously | 30            |

---

## üôè Acknowledgments

Based on the paper by Ajanthan et al., published at ICLR 2025. Inspired by the official [DeepMind async DiLoCo repo](https://github.com/google-deepmind/asyncdiloco).
"""
